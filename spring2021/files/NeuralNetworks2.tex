\documentclass[9pt]{beamer}

\beamertemplatenavigationsymbolsempty
\renewcommand\mathfamilydefault{cmr}

\usepackage{xcolor}
\usepackage{pajmath}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{intersections}
\usetikzlibrary{datavisualization}
\usetikzlibrary{datavisualization.formats.functions}
\usepackage{pgfplots}

\newcommand\VW{\V{W}}
\newcommand\bsigma{\boldsymbol{\sigma}}

\title{Neural Networks: Multi-layer Perceptrons}
\date{Spring 2021}

\begin{document}

\maketitle

\begin{frame}{Review}

\begin{itemize}
	\item A perceptron is a simplistic model of a single neuron.
	\item A perceptron can learn to perform simple classification tasks using an update rule.
	\item \textbf{Today:} Imagine what a network of millions of perceptrons can learn!
\end{itemize}
	
\end{frame}

\begin{frame}{A new activation function}

Our simple perceptron computes an intermediate value $z$ using a weighted sum of the inputs
\[ z = \V{w}\cdot\Vx \]
To simulate firing, we used the \emph{sign function} for activation:
\[ 
	y = \mathrm{sgn}(z) = \begin{cases} +1, & z > 0 \\ -1, & z < 0 \end{cases} \quad
	\raisebox{-0.45in}{
		\begin{tikzpicture}
			\draw [help lines,->] (-1,0) -- (1,0) node [below] {$z$};
			\draw [help lines,->] (0,-1) -- (0,1) node [left] {$y$};
			\draw [thick,blue] (-1,-1) node [left] {$-1$} -- (0,-1) (0,1) -- (1,1) node [right] {$+1$};
		\end{tikzpicture}
	}
\]
To avoid the discontinuity at zero, let's switch to the \emph{sigmoid function}:
\[ 
	y = \sigma(z) = \frac{1}{1 + e^{-z}} \quad
	\raisebox{-0.3in}{
		\begin{tikzpicture}
			\draw [help lines,->] (-1,0) -- (1,0) node [below] {$z$};
			\draw [help lines,->] (0,0) -- (0,1) node [left] {$y$};
			\draw [thick,blue, xscale=0.111, domain=-9:9] plot (\x, {1/(1+exp(-\x)});
			\node [blue,left] at (-1,0) {$0$};
			\node [blue,right] at (1,1) {$+1$};
		\end{tikzpicture}
	}
\]

\end{frame}

\begin{frame}{Back to logistic regression?}

A perceptron with a sigmoid activation function
\[ y = \sigma(\V{w}\cdot\Vx) \]
is equivalent to a logistic regression model.

\pause
\bigskip
Adding the sigmoid function prevents us from using the Perceptron Update Algorithm. We must use a gradient descent-type method instead (just as we did for logistic regression).

\bigskip
Fortunately, the sigmoid function has a convenient derivative:
\[ \frac{d}{dz}\sigma(z) = \sigma(z)[1 - \sigma(z)]. \]
	
\end{frame}

\begin{frame}{Multi-neuron (wide) perceptrons}

Neural networks use multiple neurons to learn different features from the \textbf{same inputs}.

\begin{center}
	\begin{tikzpicture}
		\node (a) at (0,3) {$y_1 = \sigma(\V{w}_1\cdot\Vx)$};
		\node (b) at (0,2) {$y_2 = \sigma(\V{w}_2\cdot\Vx)$};
		\node (c) at (0,1) {$y_3 = \sigma(\V{w}_3\cdot\Vx)$};
		\node (d) at (0,0) {$\vdots$};
		\node (e) at (0,-1) {$y_m = \sigma(\V{w}_m\cdot\Vx)$};
		\node (s) at (3,1) {$\Vx$};
		\draw [->,thick] (s) -- (a.east);
		\draw [->,thick] (s) -- (b.east);
		\draw [->,thick] (s) -- (c.east);
		\draw [->,thick] (s) -- (e.east);
		\onslide<2->{
			\node [blue,left] (o) at (-3,1) {$y_\text{out} = \sigma(\V{w}_\text{out}\cdot\Vy)$};
			\draw [->,blue,thick] (a.west) -- (o.north east);
			\draw [->,blue,thick] (b.west) -- (o.east);
			\draw [->,blue,thick] (c.west) -- (o.east);
			\draw [->,blue,thick] (e.west) -- (o.south east);
		};
	\end{tikzpicture}
\end{center}

\onslide<2->{
	The outputs of each neuron are collected into a single neuron to predict the final class.
}

\end{frame}

\begin{frame}{A matrix formalism for perceptrons}

Consider a stack of $m$ neurons that are all connected to the same input~\Vx.
\begin{align*}
	z_1 &= \V{w}_1\cdot\Vx \\
	z_2 &= \V{w}_2\cdot\Vx \\
	 &\vdots \\
	z_m &= \V{w}_m\cdot\Vx \\
\end{align*}

\pause
The stack can be written as the product of the input~\Vx\ and a weight matrix
\[ \Vz = \V{W}\Vx \]
where each row in $\V{W}$ contins the weights for a single neuron
\[ \V{W} = \begin{pmatrix} \leftarrow \V{w}_1 \rightarrow \\
 						   \leftarrow \V{w}_2 \rightarrow \\
 						    \vdots \\
 						   \leftarrow \V{w}_m \rightarrow \\	
 \end{pmatrix}. \]
	
\end{frame}

\begin{frame}{Three looks at linear systems}

We've studied three classes of linear systems, differing only by what is \textbf{known} and {\color{red}\textbf{unknown}}.

\bigskip
\large
\begin{align*}
	{\color{red}\Vy} &= \VA\Vx && \text{matrix multiplication} \\
	\Vy &= \VX{\color{red}\Vbeta} && \text{linear models} \\
	\Vz &= {\color{red}\V{W}}\Vx && \text{neural networks}
\end{align*}

\pause
\bigskip
\normalsize
Of these problems, neural networks are the most difficult to solve since $\V{W}$ is a \emph{matrix} of unknowns, not a \emph{vector} like \Vy\ and $\Vbeta$.
	
\end{frame}

\begin{frame}{Elementwise activation functions}

Let's define an \emph{elementwise} sigmoid activation function
\[ \bsigma(\Vz) = \begin{pmatrix} \sigma(z_1) \\ \sigma(z_2) \\ \vdots \\\sigma(z_n) \end{pmatrix}. \]
	
\pause
A stack of $m$ neurons can be written as
\begin{align*}
	\Vz &= \V{W}\Vx \\
	\Vy &= \bsigma(\Vz)	
\end{align*}
\pause
or, more succinctly as
\[ \Vy = \bsigma(\V{W}\Vx) \]
\pause
where
\[ \dim(\Vy)=m\times 1,\quad \dim(\Vz)=m\times 1 \]
\[ \dim(\V{W})=m\times n,\quad \dim(\Vx)=n\times 1 \]
\end{frame}

\begin{frame}{Completing our matrix formalism}
\begin{center}
	\small
	\begin{tikzpicture}
		\node (a) at (0,1.5) {$y_{1,1} =\sigma(z_{1,1}) \boldsymbol{\leftarrow} z_{1,1} = \V{w}_{1,1}\cdot\Vx$};
		\node (b) at (0,1) {$y_{1,2} =\sigma(z_{1,2}) \boldsymbol{\leftarrow} z_{1,2} = \V{w}_{1,2}\cdot\Vx$};
		\node (c) at (0,0.5) {$y_{1,3} =\sigma(z_{1,3}) \boldsymbol{\leftarrow} z_{1,3} = \V{w}_{1,3}\cdot\Vx$};
		\node (d) at (0,0) {$\vdots$};
		\node (e) at (0,-0.5) {$y_{1,m} =\sigma(z_{1,m}) \boldsymbol{\leftarrow} z_{1,m} = \V{w}_{1,m}\cdot\Vx$};
		\node (s) at (3,0.5) {$\Vx$};
		\draw [->,thick] (s) -- (a.east);
		\draw [->,thick] (s) -- (b.east);
		\draw [->,thick] (s) -- (c.east);
		\draw [->,thick] (s) -- (e.east);
		\node [blue,left] (o) at (-3,0.5) {$y_{2,1} =\sigma(z_{2,1}) \boldsymbol{\leftarrow} z_{2,1} = \V{w}_{2,1}\cdot\Vy_1$};
		\draw [->,blue,thick] (a.west) -- (o.north east);
		\draw [->,blue,thick] (b.west) -- (o.east);
		\draw [->,blue,thick] (c.west) -- (o.east);
		\draw [->,blue,thick] (e.west) -- (o.south east);
	\end{tikzpicture}
\end{center}

\pause
\[  \Vy_2 = \bsigma(\Vz_2) \quad\leftarrow\quad
	\Vz_2 = \V{W}_2\Vy_1 \quad\leftarrow\quad
	\Vy_1 = \bsigma(\Vz_1) \quad\leftarrow\quad
	\Vz_1 = \V{W}_1\Vx	 \]

\pause
\[ \Vy_2 = \bsigma(\V{W}_2(\bsigma(\V{W}_1\Vx))) \]

\pause
\bigskip
In general, a network with $d$ layers is
\[ \Vy_d = \bsigma(\V{W}_d(\bsigma(\V{W}_{d-1}(\cdots\bsigma(\V{W}_2(\bsigma(\V{W}_1\Vx))))))). \]

\end{frame}

\begin{frame}{Deep learning with neural networks}

\[ \Vy_d = \bsigma(\V{W}_d(\bsigma(\V{W}_{d-1}(\cdots\bsigma(\V{W}_2(\bsigma(\V{W}_1\Vx))))))) \]

\bigskip
\begin{itemize}
	\item The number of neurons in each layer $i$ is the \emph{width} of the layer.
	\begin{itemize}
		\addtolength\itemsep{0.5\baselineskip}
		\item If the ($i-1$)th layer has $n$ outputs and the $i$th layer has $m$ outputs, the weight matrix $\V{W}_i$ has dimensions $m\times n$.
		\item The dimensions of the inputs \Vx\ and outputs $\Vy_d$ are fixed by the problem.
		\item Layer $1$ is called the \emph{input layer}, and layer $d$ is the \emph{output layer}.
		\item We can use as many nodes as we want in the \emph{hidden} layers.
	\end{itemize}
	\item<2-> The number of layers $d$ is the \emph{depth} of the neural network.
	\item<3-> \emph{Deep learning} means $d>2$.
\end{itemize}
	
\end{frame}

\begin{frame}{The importance of nonlinearity}

The nonlinear functions (like $\bsigma$) sandwiched between the layers are critical to deep learning.

\bigskip
Let's imagine what would happen if we removed them:
\begin{align*}
	\Vy_d &= \bsigma(\V{W}_d(\bsigma(\V{W}_{d-1}(\cdots\bsigma(\V{W}_2(\bsigma(\V{W}_1\Vx))))))) \\
		 &= \V{W}_d(\V{W}_{d-1}(\cdots\V{W}_2(\V{W}_1\Vx))) \\
		 &= \V{W}_d \V{W}_{d-1} \cdots \V{W}_2 \V{W}_1 \Vx \\
		 &= \widetilde{\V{W}}\Vx
\end{align*}

\pause
\bigskip
Without the activation functions, the entire neural network reduces to a single linear system!
	
\end{frame}

\begin{frame}{Any nonlinearity will do}

Any nonlinear function can be an activation function.

\begin{columns}[T]
\begin{column}{0.3\textwidth}
	\begin{center}
		\textbf{Sign/step activation}
	\end{center}
\end{column}	
\begin{column}{0.3\textwidth}<2->
	\begin{center}
		\textbf{Sigmoid activation}
	\end{center}
\end{column}	
\begin{column}{0.3\textwidth}<3->
	\begin{center}
		\textbf{Rectified linear unit activation}
	\end{center}
\end{column}	
\end{columns}

\begin{columns}[T]
\begin{column}{0.3\textwidth}
	\begin{center}
		\[ \mathrm{sgn}(z) = \begin{cases} +1, & z > 0 \\ -1, & z < 0 \end{cases} \]
	\end{center}
\end{column}	
\begin{column}{0.3\textwidth}<2->
	\begin{center}
		\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
	\end{center}
\end{column}	
\begin{column}{0.3\textwidth}<3->
	\begin{center}
		\[ \mathrm{ReLU}(z) = \begin{cases} z, & z \ge 0 \\ 0, & z < 0 \end{cases} \]
	\end{center}
\end{column}	
\end{columns}


\begin{columns}[T]
\begin{column}{0.3\textwidth}
	\begin{center}
		\begin{tikzpicture}
			\draw [help lines,->] (-1,0) -- (1,0) node [below] {$z$};
			\draw [help lines,->] (0,-1) -- (0,1) node [left] {$y$};
			\draw [thick,blue] (-1,-1) node [left] {$-1$} -- (0,-1) (0,1) -- (1,1) node [right] {$+1$};
		\end{tikzpicture}
	\end{center}
\end{column}

\pause
\begin{column}{0.3\textwidth}<2->
	\begin{center}
		\begin{tikzpicture}
			\draw [help lines,->] (-1,0) -- (1,0) node [below] {$z$};
			\draw [help lines,->] (0,0) -- (0,1) node [left] {$y$};
			\draw [thick,blue, xscale=0.111, domain=-9:9] plot (\x, {1/(1+exp(-\x)});
			\node [blue,left] at (-1,0) {$0$};
			\node [blue,right] at (1,1) {$+1$};
		\end{tikzpicture}
	\end{center}
\end{column}

\pause
\begin{column}{0.3\textwidth}<3->
	\begin{center}
		\begin{tikzpicture}
			\draw [help lines,->] (-1,0) -- (1,0) node [below] {$z$};
			\draw [help lines,->] (0,0) -- (0,1) node [left] {$y$};
			\draw [thick,blue] (-1,0) -- (0,0) -- (1,1);
			\node [blue,left] at (-1,0) {$0$};
			\node [blue,right] at (1,1) {$z$};
		\end{tikzpicture}
	\end{center}
\end{column}

\end{columns}	
\end{frame}

\begin{frame}{Why do we want deep networks?}

\begin{itemize}
	\item The \emph{Universal Approximation Theorem} states that given enough neurons, a 2-layer (input/output) perceptron can learn any reasonable function.
	\item Neural networks are therefore universal function approximators.
\end{itemize}

\pause
\begin{itemize}
	\item Unfortunately, the theorem does not tell us how many neurons we need to approximate a given function.
	\item For complicated functions, evidence suggests the number is enormous!
\end{itemize}

\pause
\begin{itemize}
	\item Our brains are very deep, so it's reasonable to believe that deep networks learn more efficiently than wide ones.
	\item In practice this is almost certainly true.
	\item Deep learning reduces the total number of neurons needed to learn a function since each of the $d$ layers needs fewer than $1/d$-times the number of neurons.
\end{itemize}
	
\end{frame}

\begin{frame}{Why do deeper networks learn better?}
We can understand deep networks using examples from \emph{feature engineering}.

\bigskip
Imagine you wanted to learn a Michaelis-Menten function.
\begin{columns}
\begin{column}{0.4\textwidth}<2->
	\begin{center}
	\textbf{Single Layer}
	\begin{tikzpicture}[arr/.style={black,thick,->}]
		\node (Vmax) at (1,3) {$V_\text{max}$};
		\node (S) at (1,2) {$[S]$};
		\node (Km) at (1,1) {$K_m$};
		\node (full) at (-1.5,2) {$\displaystyle \frac{V_\text{max}[S]}{K_m+[S]}$};
		\draw [arr] (Vmax) -- (full);
		\draw [arr] (S) -- (full);
		\draw [arr] (Km) -- (full);
	\end{tikzpicture}	
\end{center}
\end{column}

\begin{column}{0.6\textwidth}<3->
	\begin{center}
	\textbf{Two Layers}
	\begin{tikzpicture}[arr/.style={black,thick,->}]
		\node (Vmax) at (3,3) {$V_\text{max}$};
		\node (S) at (3,2) {$[S]$};
		\node (Km) at (3,1) {$K_m$};
		\node (VmaxS) at (1,2.5) {$V_\text{max}[S]$};
		\node (KmS) at (1,1.5) {$K_m + [S]$};
		\node (full) at (-1.5,2) {$\displaystyle \frac{V_\text{max}[S]}{K_m+[S]}$};
		\draw [arr] (Vmax) -- (VmaxS);
		\draw [arr] (S) -- (VmaxS);
		\draw [arr] (Km) -- (KmS);
		\draw [arr] (S) -- (KmS);
		\draw [arr] (VmaxS) -- (full);
		\draw [arr] (KmS) -- (full);
	\end{tikzpicture}	
\end{center}
\end{column}
	
\end{columns}

\bigskip
\onslide<4->{
	Each layer in the network only needs to improve the features for the next layer.
}

\end{frame}

\begin{frame}{Summary}

\begin{itemize}
	\addtolength\itemsep{0.5\baselineskip}
	\item Deep neural networks are built from layers in artificial neurons.
	\item Each neuron has the power of a linear classifier.
	\item Layers \textbf{must} be separated by nonlinear activation functions.
	\item Neural networks can learn nearly any function, but deep networks learn more efficiently.
	\item Each layer creates features for the subsequent layers to improve learning.
\end{itemize}
	
\end{frame}


\begin{frame}{Preparing for gradient descent}

Neural networks are trained with a variation of gradient descent. Before calculating the gradients, we need to review the chain rule from calculus.

\bigskip
\pause
For any differentiable functions $f$ and $h$,
\[ \frac{d}{dx}h(f(x)) = \frac{dh}{df}\frac{df}{dx} \]

\pause
Similarly, for the $k$ functions $f_1$, $f_2$, $\ldots$, $f_k$,
\[ \frac{d}{dx}f_k(f_{k-1}(\cdots f_2(f_1(x)))) = \frac{df_k}{df_{k-1}} \frac{df_{k-1}}{df_{k-2}}\cdots \frac{df_i}{df_{i-1}}\cdots \frac{df_2}{df_1}\frac{df_1}{dx} \]

\pause
For example, let $f(x) = (2x-3)^3$. Let $f_1(x) = 2x-3$ and $f_2(z) = z^3$. Then $f(x) = f_2(f_1(x))$ and
\begin{align*}
	\frac{df}{dx} &= \frac{df_2}{df_1}\frac{df_1}{dx} \\
		&= 3(2x-3)^2(2)
\end{align*}
\end{frame}


\end{document}
