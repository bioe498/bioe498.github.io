\documentclass[9pt]{beamer}

\beamertemplatenavigationsymbolsempty
\renewcommand\mathfamilydefault{cmr}

\usepackage{pajmath}
\usepackage{booktabs}
\usepackage{colortbl}

\usepackage{tikz}
\usetikzlibrary{positioning,shapes.misc,calc,backgrounds,scopes} 
\usetikzlibrary{datavisualization}
\usetikzlibrary{datavisualization.formats.functions}
\tikzset{boxed/.style={
  thick,
  draw=black,
  top color=white,
  text height=1.5ex,
  text depth=.25ex
}}


\newcommand\lo{$-1$}
\newcommand\hi{$\phan1$}
\newcommand\ze{$\phan0$}
\newcommand\Ze{$\phan\Vzero$}
\newcommand\pskip{\pause\bigskip}
\newcommand\lspace{\addtolength{\itemsep}{0.5\baselineskip}}
\newcommand\red[1]{{\color{red}#1}}

\title{Reinforcement Learning:\\$Q$-learning and AlphaGo}
\author{BIOE 498/598 PJ}
\date{Spring 2022}

\begin{document}
\frame{\titlepage}


\begin{frame}{Review}

\begin{itemize}\lspace
	\item Discount factors shorten the horizon of RL problems, causing the agent to focus on rewards in the near future.
	\item Temporal Difference (TD) learning incrementally updates value functions using a new experience.
	\item Learning $Q$-factors eliminates the need to predict the next state given an action; however, the number of $Q$-factors is much greater than the number of states.
	\item<2-> \textbf{Today:}
		\begin{itemize}\lspace
			\item Review SARSA
			\item $Q$-learning
			\item AlphaGo
		\end{itemize}
\end{itemize}
	
\end{frame}

\begin{frame}{Learning $Q$-factors}

Using $Q$-factors, the policy problem at state~$s_i$
	\[ \max_{a} \mathbb{E}\left\{ r_i + \gamma V(s_{i+1}) \right\} \]
becomes
	\[ \max_{a} \mathbb{E}\left\{ Q(s_i,a) \right\}. \]

\pskip
\begin{itemize}
	\item \textbf{Pro:} We do not need a model or a way to predict $s_{i+1}$.
	\item \textbf{Con:} We need to learn a $Q$-factor for every state/action pair.
\end{itemize}

\pskip
We can learn $Q$-factors using a TD approach given a trajectory $s_0,a_0,r_0$, $s_1,a_1,r_1$ $\ldots$, $s_T,r_T$:
\begin{align*}
	\hat{Q}(s_i,a_i) &= r_i + \gamma Q(s_{i+1},a_{i+1}) & \text{target} \\
	Q(s_i,a_i) &\leftarrow Q(s_i,a_i) + \alpha\left[ \hat{Q}(s_i,a_i) - Q(s_i,a_i) \right] & \text{update}
\end{align*}
This approach is called \emph{SARSA}.

\end{frame}

\begin{frame}{SARSA follows a trajectory, not an optimal path}

The SARSA update equation is

\[ Q(s_i,a_i) \leftarrow Q(s_i,a_i) + \alpha\bigg[ \underbrace{r_i + \gamma Q(s_{i+1},a_{i+1})}_\text{target} - Q(s_i,a_i) \bigg]. \]

\pskip
Our estimate of $Q(s_i,a_i)$ is based on
\begin{itemize}
	\item The reward $r_i$ experienced by selecting action $a_i$ in state $s_i$.
	\item The future reward $Q(s_{i+1},a_{i+1})$ based on the action $a_{i+1}$ from the trajectory.
\end{itemize}

\pskip
The policy that generated the trajectory is not optimal, so it is likely that $a_{i+1}$ was not the best action to take.

\bigskip
Selecting a suboptimal action underestimates the reward to go, and therefore the value $Q(s_i,a_i)$.

\end{frame}

\begin{frame}{$Q$-learning}

The $Q$-learning algorithm changes the SARSA update
\[ Q(s_i,a_i) \leftarrow Q(s_i,a_i) + \alpha\left[ r_i + \gamma Q(s_{i+1},a_{i+1}) - Q(s_i,a_i) \right] \]
to use the optimal action in state $s_{i+1}$:
\[ Q(s_i,a_i) \leftarrow Q(s_i,a_i) + \alpha\left[ r_i + \gamma\, \red{\max_a Q(s_{i+1},a)} - Q(s_i,a_i) \right]. \]
	
\pskip
$Q$-learning can converge faster to an optimal policy. However, it has two drawbacks:
\begin{enumerate}
	\item If the number of available actions is large, the maximization operator can be expensive to evaluate.
	\item The maximization operator is biased.
\end{enumerate}
\end{frame}

\begin{frame}{Records were meant to be broken.}

\begin{itemize}\lspace
	\item Imagine that the quality of professional basketball players was fixed over time.
	\item In this case, scoring records would still be broken.
	\item Basketball includes stochastic elements, so as more games are played the chance of observing outliers increases.
\end{itemize}

\pskip
Any algorithm with a $\max$ operator will drift upwards over time, \emph{even if the mean value remains fixed}.

\bigskip
For $Q$-learning, we need to combat the bias in the $\max$ operator.
	
\end{frame}

\begin{frame}{Double $Q$-learning}

One solution to the $\max$ bias is using two separate $Q$ functions (networks), called $Q_1$ and $Q_2$.

\bigskip
Both $Q_1$ and $Q_2$ are trained with separate experiences. (Or, one network can \emph{lag} behind the other in experiences.)

\pskip
When updating, we use one network to select the action, and the other network to compute its value.
\begin{align*}
	Q_1(s_i,a_i) &\leftarrow Q_1(s_i,a_i) + \alpha\left[ r_i + \gamma\, Q_2(s_{i+1},a_1) - Q_1(s_i,a_i) \right] \\
	& \quad a_1 \equiv \arg\max_a Q_1(s_{i+1},a) \\ \\
	Q_2(s_i,a_i) &\leftarrow Q_2(s_i,a_i) + \alpha\left[ r_i + \gamma\, Q_1(s_{i+1},a_2) - Q_2(s_i,a_i) \right] \\
	& \quad a_2 \equiv \arg\max_a Q_2(s_{i+1},a)
\end{align*}

\pskip
Even if $a_1$ was selected because $Q_1(s_{i+1},a_1)$ was aberrantly high, the value $Q_2(s_{i+1},a_1)$ will not share this bias.
	
\end{frame}

%\begin{frame}{Deep $Q$-learning}
%
%\begin{itemize}\lspace
%	\item Currently, the most common method for approximating $Q$-factors is \emph{deep learning} with artificial neural networks.
%	\item We're going to learn to play a simple board game called Tic-Tac-Go.
%	\item We want a game that is simple enough to be computationally tractable, but not easily solved.
%	\item<2->Tic-Tac-Toe is simple, but solved. If both players follow an optimal strategy, the game will always end in a draw.
%	\item<3->Go is unsolved, but approximating $Q$-factors is ridiculously expensive.
%\end{itemize}
%
%\end{frame}


\begin{frame}{Summary}

\begin{itemize}\lspace
	\item $Q$-learning is a state-of-the-art technique for RL.
	\item Double $Q$-learning counteracts the bias in the $\max$ operator.
\end{itemize}
	
\end{frame}


\end{document}
