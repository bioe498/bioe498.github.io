---
title: "Analysis of Variance"
author: "BIOE 498/598"
date: "2/19/2020"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Does our model do anything?
Let's return to our data of my son throwing the stuffed monkey.
\tiny
```{r}
attach(read.csv("AndersThrow.csv"))
model <- lm(distance ~ 0 + hand + hat + boots)
summary(model)
```

## The sum of squares

Our analysis is based on the *sum of squares*, or SS. In particular, the total SS is the combination of the SS explained by our model and the SS that is residual (or unexplained).

\newcommand\sst{SS_\mathrm{total}}
\newcommand\ssx{SS_\mathrm{explained}}
\newcommand\ssr{SS_\mathrm{residual}}

\[ \sst = \ssx + \ssr \]

\newcommand\y{\mathbf{y}}
\newcommand\mean{\mathrm{mean}}
\newcommand\pred{\mathrm{predicted}}

\pause
How do we calculate each one for a model $\y = \mathbf{X}\mathbf{\beta}$?

\pause
\[ \sst = \sum_i (y_i - \mean(\y))^2 \]
\pause
\[ \ssr = \sum_i (y_i - \pred(y_i))^2 \]
\pause
\[ \ssx = \sst - \ssr \]

## For our throwing data

```{r}
ss <- function(x) sum(x^2)
sst <- ss(distance - 0)
ssr <- ss(residuals(model))
ssx <- sst - ssr
c(sst, ssr, ssx)
```

## Degrees of freedom

The amount of variation we see depends on the number of independent parameters in the model. These are the *degrees of freedom*.

- For $\ssx$, DF = # of parameters
- For $\ssr$, DF = (# data points) - (# parameters)

## The $F$-statistic

The value of our model is explained by the ratio between the explained variance and the residual (unexplained) variance *after adjusting for the DF*.

\[ F = \frac{\ssx / \mathrm{DF}(\ssx)}{\ssr / \mathrm{DF(\ssr)}} \]

\pause
For our throwing example

\[ F = \frac{329.125 /4}{5.875 / (8-4)} = 56.02 \]

## How big should the $F$-statistic be?

That depends on the number of degrees of freedom. The $F$-statistic follows the $F$-distribution. We can use this distribution to convert the $F$-statistic into a $p$-value.
\tiny
```{r}
summary(model)
```

## Testing single factors

We previously compared the entire model against the residuals to see if the model added value. We can apply the same procedure to a single variable.

This is called the *analysis of variance*, or ANOVA.

## ANOVA on handedness

Let's find the explained variance for a model with only handedness:
\small
```{r}
model_hand <- lm(distance ~ 0 + hand)
sst - ss(residuals(model_hand))
```
\normalsize
Now let's compare this to the residuals of the entire model:

\pause
\[ F = \frac{322.625 /2}{5.875 / (8-4)} = 109.83 \]

## ANOVA on a linear model

We can repeat this procedure for every variable, or we can use R's built-in ANOVA command.
```{r}
anova(model)
```

## Conclusions

- $p$-values on effect sizes tell us if the effect size in nonzero.
- A significant effect size does not mean the effect matters.
- ANOVA can tell us which variables (not parameters!) explain a significant fraction of the variance in our data.
- Significance is realtive to the unexplained variance in the model.




